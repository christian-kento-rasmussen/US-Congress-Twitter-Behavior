{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a411e1fb347c47d691b27e08135dcff0","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Explainer-Notebook"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"0b2a1dcd1efa4c59912fdcc0fd59aa81","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1683392595390,"source_hash":"56105fbc"},"outputs":[],"source":["import json\n","import pandas as pd\n","import os\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import random\n","from scipy import stats\n","import networkx as nx\n","import pandas as pd\n","from collections import defaultdict\n","import pickle\n","from ast import literal_eval\n","import pickle\n","import netwulf as nw"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"4103a11afb0a401f9feff798b5739932","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Motivation"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a7a855356e3d49f2954fad306b99fe2a","deepnote_cell_type":"markdown"},"source":["* What is your dataset?\n","* Why did you choose this/these particular dataset(s)?\n","* What was your goal for the end user’s experience?"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"5cffc4dec1a4449e92027e99ce5328d7","deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false},"source":["### What is the data?"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"aee5af1900fa4752950bda6de3a7748c","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Raw data"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"5e3e3476-e077-4343-b977-44ee16e42fe3","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The raw data consists of a range of tweets made by users all related to the U.S congress  either as members, committees or caucus, with the tweets' IDs, the user's handle names, timestamp, a link to the tweet and finally the text of the tweet.  The tweets were taken daily from 2017-2023 and were arranged in JSON files by date with a time resolution of months."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"44502003-aeb0-4392-a532-473dc89fa11a","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["We also had a JSON file called \"historical users\" consisting of data on the different twitter users. It contained the name of the user, which chamber they were a part of, their political affiliation, what type of user they were (comittee, political party, member or caucus) and a list of which twitter accounts were associated with that user. Furthermore, if the user was a member of congress they would also have a field explaining which state they represented. "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"be76dbd3-d3ad-47aa-871c-a8599fff8228","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["10 of the members had a prev_props field, which indicated which political party, if not their own, they had served and when."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f5a41c99-c38c-4e67-b098-6e14934314b8","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The accounts list consisted of dictionaries with the handle names, what the accounts had been used for (campaign, office etc.) and unique IDs of the different twitter accounts belonging to that person. Some of the twitter accounts also had old handle names, which were included in the accoutns list."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d2bf7b2c-8195-46ff-98d6-17835b0836db","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Basic stats. Let’s understand the dataset better"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* Write about your choices in data cleaning and preprocessing\n","* Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d7509f49ccef435881808aa2bd710e8e","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Write about your choices in data cleaning and preprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data frame"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We made multiple choices when extracting and proccessing the data from our raw JSON file, as described above."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["User dataframe:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Removing prev_pres: As mentioned, there was an attribute called \"prev_pres,\" which, if the member had been elected for a previous party or house, described this party or house. However, only 10 members in total had this attribute, so we decided to consider it as an outlying attribute and removed it from our graph. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Tweets dataframe:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["-  Multiple mentions in the same tweet: One could argue, that multiple mentions in the same tweet should account for a single edge, as it could be viewed as a single instance of communication. However, at the same time, one most likely does not accidentally mention another twitter user, and there is therefore en explicit attempt at connection between the mentioner and the mentionee. As we're researching how the different congress members relate to one another, it would not make sense to throw away this information, despite the fact, that it may cause overrepresentation of certain individuals who're connected with other individuals that use a lot of mentions in their tweets compared to the average."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Only including mentions to accounts in our accounts dataframe: We did not include mentions directed at- or from accounts outside our accounts dataframe, as we would have no political information on these accounts, the most important being which party they belonged to. It would therefore most likely not make sense to include these extra edges and nodes, as it would just unnecessarily increase the complexity of our graph, and not help us answer our main research question regarding whether or not the U.S Congress is politically polarized."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Include RT tweets: We included retweets as we saw them as a piece of information regarding the relationship between two users. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We had four dataframes; User, accounts, tweets and text dataframe."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["User dataframe: A dataframe of the bibliographic data sorrounding a single twitter user, a user being the organization or persons(s) actually using twitter."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Size: 806 entries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- name: This was the unique identifier of each user as no user had the same exact name."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- chamber: Which chamber (if any,) the user was a part of, could either be house or congress."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- type: Whether they were a (congress) member, caucus, party or committee"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- party: Which political party they were party of"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- id: Their ID"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- state: Which state they represented, if they were a member of a congress"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- prev_props: 10 users had served under different parties previously, prev_props told us which parties and the timeframe."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'filteredDataFilePath' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n","Cell \u001b[0;32mIn [2], line 5\u001b[0m\n","\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Create the tweeters fundamental DB with:\u001b[39;00m\n","\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mname, chamber, type, party, (list) accounts, id, state, prev_props\u001b[39;00m\n","\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n","\u001b[0;32m----> 5\u001b[0m users_DF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[43mfilteredDataFilePath\u001b[49m)\n","\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Check for duplicate names\u001b[39;00m\n","\u001b[1;32m      8\u001b[0m names_list \u001b[38;5;241m=\u001b[39m users_DF[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\n","\u001b[0;31mNameError\u001b[0m: name 'filteredDataFilePath' is not defined"]}],"source":["\"\"\"Create the tweeters fundamental DB with:\n","name, chamber, type, party, (list) accounts, id, state, prev_props\n","\"\"\"\n","\n","users_DF = pd.read_json(filteredDataFilePath)\n","\n","#Check for duplicate names\n","names_list = users_DF[\"name\"]\n","duplicate_names_counter = 0\n","for name in names_list:\n","    name_query = users_DF[users_DF[\"name\"] == names_list[0]]\n","    if(len(name_query)>1):\n","        print(name_query)\n","        duplicate_names_counter += 1\n","\n","if(duplicate_names_counter>0):\n","    print(f\"Number of duplicate names: {duplicate_names_counter}\")\n","else:\n","    print(\"We can use name as a unique identifier for Twitter users\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Accounts dataframe: Each user could have multiple accounts. The accounts dataframe was therefore a way to link twitter accounts' handles and ids to the user behind them."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here each row is a single twitter account."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Size: 1754 entries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- screen_name: The username of that account"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- account_type: What the account was used for, i.g campaign, office etc."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- name: The name of the user behind the account"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","We create a seperate dataFrame for quickly connecting twitter handles with names.\n","We need this in order to iterate over the tweets and filter mentions based on whether they're part of our filtered dataset.\n","\"\"\"\n","\n","#Get all the account dicts as rows\n","accounts_dataFrame = users_DF[[\"accounts\"]].explode([\"accounts\"])[\"accounts\"].apply(pd.Series)\n","users_DF = users_DF.drop(\"accounts\",1)\n","\n","accounts_dataFrame[\"name\"] = users_DF[\"name\"]\n","accounts_dataFrame = accounts_dataFrame.drop([\"party\",\"type\",\"deleted\",\"chamber\"],1)\n","\n","#As can be seen some of the twitter accounts have had previous names, which we further need to explode and add to the dataframe\n","#We swap screen_names with prev_names, and add the new connections between names and twitter handles to the original accounts_DF\n","\n","prev_names_DF = accounts_dataFrame[accounts_dataFrame[\"prev_names\"].isna() == False].explode(\"prev_names\").drop(\"screen_name\",1)\n","\n","prev_names_DF.columns = [\"id\",\"account_type\",\"screen_name\",\"name\"]\n","accounts_dataFrame = pd.concat([accounts_dataFrame,prev_names_DF]).drop(\"prev_names\",1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Tweets dataframe:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The tweets dataframe stores information surrounding a tweet. This made it easy for us to create a graph later on, as all the edges could be found in the tweets dataframe. Each entry is a single tweet."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Size: 4777249 entries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- user_name: The username of the user behind the tweet"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- screen_name: The name of the account from which the tweet was posted"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- tweet_account_ID: The ID of that account"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- tweet_ID: The unique identifier identifying that single tweet"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- handles_mentioned: All the twitter handles mentioned, which are in our account dataframe."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- names_mentioned: The username of the accounts mentioned."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Text dataframe:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The text dataframe consists of all the text of every tweet in the tweets dataframe. The primary purpose of this dataframe was to provide an organized way to do text analysis."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each row is a single tweet."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Size: 4777249 entries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- user_name: The username of the user behind the tweet"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- screen_name: The name of the account from which the tweet was posted"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- text: The actual text of that tweet"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- tweet_ID: The unique identifier identifying that single tweet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RT_pattern = \"RT @[a-zA-Z0-9]*\"\n","\n","def searchText(formula,twitter_message):\n","    try:\n","        matches = re.findall(formula,twitter_message)\n","        matches[0]\n","        return matches\n","    except:\n","        return None\n","\n","#Takes a tweet and returns a list of mentions - bar RT mentions - that are in our accounts_dataFrame and their corresponding users' names\n","def get_Handles_Names(tweet):\n","    mentions_re = \"@[a-zA-Z0-9]*\"\n","    mentions_excl_RT_re = \"(?<!RT )@[a-zA-Z0-9]*\" #Will exclude all mentions that follow \"RT\"\n","    mentions = searchText(mentions_re,tweet)\n","    screen_names = []\n","    names_result = []\n","\n","    #If there is a mention\n","    if(mentions != None):\n","        mentions = [handle[1:] for handle in mentions] #Remove @\n","\n","        #Add to our dict, if the mentions exist in our dataFrame\n","        for idx,handle in enumerate(mentions):\n","            names = accounts_dataFrame[accounts_dataFrame['screen_name'] == handle]['name']\n","\n","            #If the handle exists in our filtered accounts_dataFrame\n","            if(len(names)>0):\n","                screen_names += [handle]\n","                names_result += list(names)\n","    if(len(names_result)>0):\n","        return screen_names,names_result\n","    else:\n","        return None,None\n","\n","def get_tweeter_screen_name(tweet):\n","    return tweet[\"screen_name\"]\n","\n","\n","\n","#In case the way we accept tweets changes\n","def accept_tweet(tweet):\n","    screen_name = [get_tweeter_screen_name(tweet)]\n","\n","    #We check to see if the sender is a part of our network\n","    if(accounts_dataFrame[\"screen_name\"].isin(screen_name).sum() == 1):\n","        return True\n","    else:\n","        return False\n","\n","\n","\n","def get_tweets_row(tweet,year):\n","        screen_name = get_tweeter_screen_name(tweet)\n","        tweet_text = tweet[\"text\"]\n","        tweet_ID = tweet[\"id\"]\n","        tweet_account_ID = tweet[\"user_id\"] #What they call user, we call account\n","\n","        handles_mentioned,names_mentioned = get_Handles_Names(tweet_text)\n","        hashtags = searchText(hashtags_re,tweet_text)\n","        user_name = list(accounts_dataFrame[accounts_dataFrame[\"screen_name\"] == screen_name][\"name\"])[0]\n","\n","        return {\"user_name\":user_name,\"screen_name\":screen_name,\"tweet_account_ID\":tweet_account_ID,\"tweet_ID\":tweet_ID,\"handles_mentioned\":handles_mentioned,\"names_mentioned\":names_mentioned,\"hashtags\":hashtags,\"year\":year}\n","\n","def get_text_row(tweet):\n","        screen_name = get_tweeter_screen_name(tweet)\n","        user_name = list(accounts_dataFrame[accounts_dataFrame[\"screen_name\"] == screen_name][\"name\"])[0]\n","        tweet_text = tweet[\"text\"]\n","        tweet_ID = tweet[\"id\"]\n","        return {\"user_name\":user_name,\"screen_name\":screen_name,\"text\":tweet_text,\"tweet_ID\":tweet_ID}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"We will now create the Tweets and text dataframes\"\"\"\n","\n","tweets_DF = pd.DataFrame([\"user_name\",\"screen_name\",\"tweet_account_ID\",\"tweet_ID\",\"handles_mentioned\",\"names_mentioned\",\"hashtags\"])\n","\n","hashtags_re = \"#[a-zA-Z0-9]*\"\n","\n","#Regex assumes \\ is an escape char, Python assumes \\\\ is an escape char for a single \\, we need \\\\ (Regex) -> \\\\\\\\ (Python)\n","year_re = \"\\\\\\\\[0-9]*-\"\n","tweets_rows = []\n","text_rows = []\n","false_tweets = 0\n","total_tweets = 0\n","\n","for idx,filename in enumerate(os.listdir(dataFilePath)):\n","    print(idx)\n","\n","    full_path = os.path.join(dataFilePath, filename)\n","    tweet_year = re.search(year_re,full_path)[0][1:-1]\n","\n","    with open(full_path, encoding=\"utf8\") as f:\n","        json_file = json.load(f)\n","\n","    try:\n","        for tweet in json_file:\n","            if(accept_tweet(tweet)):\n","                tweets_rows += [get_tweets_row(tweet,tweet_year)]\n","                text_rows += [get_text_row(tweet)]\n","                total_tweets += 1\n","\n","    except Exception as e:\n","        false_tweets += 1\n","        print(tweet)\n","        print(e)\n","\n","#We now have our four different dataFrames.\n","tweets_DF = pd.DataFrame.from_dict(tweets_rows,orient=\"columns\")\n","text_DF = pd.DataFrame.from_dict(text_rows,orient=\"columns\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main Graph creation"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d10f58af-6a66-45bb-b14f-950ed577c287","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Directed: The choice of using a directed graph versus an undirected graph stems from the fact, that there is a huge difference between a user who mentions a lot of other users, or whom is mentioned by a lot of other users. With the ladder playing a more central role in our network."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"447da55e-ad78-43fc-92fb-263c5a462821","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Multigraph: We couldn't just assign number of tweets between two nodes as weights, as we needed to separate the graph by year later on, and we would therefore have to mark which how many mentions were distributed in each year. Also, originally we had intended to use semantic analysis, and by using the multigraph, we could have a single weight for semantics. This would make it much easier to use the standard tools we had, like Netwulff and NetworkX."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"ed1ab804-7ab5-4b9a-b293-5a80b29f50b3","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Users are nodes, not accounts: For the choice of using twitter users instead of their accounts as the nodes of our graph, we went with the twitter users. Having the accounts as nodes would've added more detail, as there could now be cases where the same users had accounts in different communities. Or, we could see how the different election periods affected the use of different accounts. However, it would also add much more complexity to our graph, and it could have had a possibility of making it much harder to get an overview of our network based off of visual cues, like our Netwulff graph. As we had limited time for this assignment, we therefore went with the former option."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#We now create the network graph\n","\n","\n","#When we load the dataFrame, it interprets the names_mentioned as a string, not a list\n","def read_names_mentioned(tweet):\n","    return literal_eval(tweet[\"names_mentioned\"])\n","\n","def load_DF(path):\n","    return pd.read_csv(path).drop(\"Unnamed: 0\",1)\n","\n","#And load the different dataFrames\n","tweets_DF = load_DF(\"tweetsDF\")\n","text_DF = load_DF(\"textDF\")\n","users_DF = load_DF(\"userDF\")\n","accounts_dataFrame = load_DF(\"accountsDF\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After having created our different dataframes, we now reached the last data extraction state."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We wanted to model the relationship between the different of the US congress, not their twitter accounts, so each node in our graph consists of a single user and its data from the user dataframe."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In order to construct the actual graph, we started by constructing the edges by looking at each tweet from our tweets dataframe that mentioned at least one other user from the account dataframe. We then created an edge between the user who tweeted the tweet and each mention in that tweet. If the same user was mentioned multiple times, there would be created an edge for every mention."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we wanted to do text analysis on the communities of the graph and a temporal analysis later on, we added the year of the tweet as well as the tweet's ID to every edge. This way we could look at all the edges of each community and get the text from those tweets from the text dataframe, and we could separate the different edges into different graphs by year."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weets_w_edges = tweets_DF[tweets_DF[\"names_mentioned\"].isna() == False]\n","#%%\n","#First we create the edges\n","edges = []\n","for idx in range(len(tweets_w_edges)):\n","    tweet = tweets_w_edges.iloc[idx]\n","    origin = tweet[\"user_name\"]\n","    name_mentions = read_names_mentioned(tweet)\n","    try:\n","        for name_mention in name_mentions:\n","            if (origin != name_mention):\n","                edges += [(origin, name_mention,{\"year\":tweet[\"year\"],\"ID\":tweet[\"tweet_ID\"]})]\n","                if (len(name_mention) == 1):\n","                    print(f\"Origin:{origin} and the mentioned: {name_mention}\")\n","    except:\n","        print(tweet)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We then created a directed multigraph based on the edges. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The next step was adding the attributes of each node:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Now we add our attributes to the nodes in the graph\n","attribute_dicts = dict()\n","\n","#For each node, we create an attributes dict\n","for idx in range(len(users_DF)):\n","    row = users_DF.iloc[idx]\n","    node_dict = dict()\n","\n","    #We take all the attributes but name and id\n","    for attr in users_DF.drop([\"name\",\"id\"],1).columns:\n","        value = row[attr]\n","        node_dict[attr] = value\n","\n","    attribute_dicts[row[\"name\"]] = node_dict\n","\n","nx.set_node_attributes(G,attribute_dicts)\n","print(f\"A quick litmus test: {G.nodes['Bernie Sanders']}\\nThe values should be equal to senate, member, I, VT and nan\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Finally, we created a directed multigraph for every year we had tweets. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For year i, if a node had edges from- or to itself with the attribute year = i, the two nodes and the edge was included in the graph for that year."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Finally we create a graph for every year there is a tweet\n","years = nx.get_edge_attributes(G, \"year\")  # Get the year attribute for each edge\n","graph_by_year = {}  # Dictionary to store subgraphs by year\n","\n","\n","for edge in G.edges(data=True):\n","    year = edge[2][\"year\"]\n","\n","    # Create a new subgraph for each year\n","    if year not in graph_by_year:\n","        graph_by_year[year] = []\n","\n","    graph_by_year[year] += [edge]# Add the edge to the respective subgraph\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#%%\n","#We now need to populate the nodes' attributes\n","\n","#We were not succesful in extracting a subgraph based on the edges directly,\n","#or using the nx.set_node(), so we're just gonna iterate over all the nodes and set their attributes \"manually\"\n","all_node_attributes = {\"chamber\":None,\"type\":None,\"party\":None,\"state\":None,\"prev_props\":None}\n","\n","for idx,year in enumerate(graph_by_year.keys()):\n","\n","    yearGraph = nx.MultiDiGraph(graph_by_year[year])\n","\n","    #First we populate our \"all_node_attributes\"\n","    for attr in all_node_attributes.keys():\n","        all_node_attributes[attr] = nx.get_node_attributes(G,attr)\n","\n","    #We then iterate over all the attributes and set them in our graph\n","    for node_attr in all_node_attributes.keys():\n","\n","        #single node attribute, like chamber,type, etc.\n","        single_node_attribute = all_node_attributes[node_attr]\n","\n","\n","        for node in single_node_attribute.keys():\n","            if(node in yearGraph.nodes):\n","                yearGraph.nodes[node][node_attr] = single_node_attribute[node]\n","\n","    pickle.dump(yearGraph, open(f'directed_multi_twitter_graph_fixed{year}.pickle', 'wb'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"0a323061ee3346eda993ee5e3310c61f","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Basic data statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6dbfd85b03dd44e49b475456640b796b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1489262123,"execution_start":1681902392109,"source_hash":"c1f117e8"},"outputs":[],"source":["print(DiG)\n","print(nx.density(DiG))\n","\n","degree_list = list(DiG.degree())\n","degree_list = [degree_tuple[1] for degree_tuple in degree_list]\n","print(f\"average {np.mean(degree_list)}, \\nmedian {np.median(degree_list)}, \\nmode {stats.mode(degree_list)[0][0]}, \\nminimum {np.min(degree_list)}, \\nmaximum {np.max(degree_list)} \\nvalue of the degree\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"dcc22ee0487f420eb5948c00e2f90ccc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1488964735,"execution_start":1681902689500,"source_hash":"72ecac2b"},"outputs":[],"source":["in_degrees = [degree for note, degree in DiG.in_degree()]\n","out_degrees = [degree for note, degree in DiG.out_degree()]\n","print(f\"In degree: average {np.mean(in_degrees):0.1f}, \\tmedian {np.median(in_degrees)}, \\tmode {stats.mode(in_degrees,keepdims=False)[0]}, \\tminimum {np.min(in_degrees)}, \\tmaximum {np.max(in_degrees)} \")\n","print(f\"Out degree: average {np.mean(out_degrees):0.1f}, \\tmedian {np.median(out_degrees)}, \\tmode {stats.mode(out_degrees,keepdims=False)[0]}, \\tminimum {np.min(out_degrees)}, \\tmaximum {np.max(out_degrees)}\")\n","\n","fig, ax = plt.subplots(1,2,figsize=(10,4))\n","\n","ax[0].hist(in_degrees,bins=30)\n","ax[0].set_title('In Degree distribution of DiG')\n","ax[0].set_ylabel('count')\n","ax[0].set_xlabel('degree')\n","\n","ax[1].hist(out_degrees,bins=30)\n","ax[1].set_title('Out Degree distribution of DiG')\n","ax[1].set_ylabel('count')\n","ax[1].set_xlabel('degree')\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"cc6f0c59741a430bb7c0dcd459c67584","deepnote_cell_type":"markdown"},"source":["We then wanted to look at the users with the highest in and out degree"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"24ed8f6f9103465687f8caaedc502806","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1488342124,"execution_start":1681903312111,"source_hash":"86c2882"},"outputs":[],"source":["in_degrees_sorted = [(node, degree) for node, degree in DiG.in_degree()]\n","in_degrees_sorted.sort(key=lambda x: x[1], reverse=True)\n","out_degrees_sorted = [(node, degree) for node, degree in DiG.out_degree()]\n","out_degrees_sorted.sort(key=lambda x: x[1], reverse=True)\n","\n","\n","print(\"in degrees sorted : \\n\", in_degrees_sorted[:5])\n","print(\"out degrees sorted : \\n\", out_degrees_sorted[:5])"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d569a6202def4832b12c3da6f968f037","deepnote_cell_type":"markdown"},"source":["From this we can see that the most mentioned twitter account for all years are for 'VP' and 'SpeakerPelosi'. This comes as no suprise beause they are some of the main twitter accounts for the united states goverment and they have been activate for all years we have data for.\n","\n","For the out degree we have 'RepDonBeyer', 'RepDonBacon and 'auctnr1' which is the account for 'Billy Long'. They are all very activate republian twitter users. That averages multiple tweets each day. 'NRCC' is another interesting account, beacuse it is not a person but a group as stated on the twitter bio: \"The NRCC is dedicated to defending our conservative majority in the House\". "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"6b4a8903-b553-43e4-b040-7dd4146d1293","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"0aea8f6e47444c418256ea11f52ce26c","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Tools, theory and analysis. Describe the process of theory to insight"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"86a1acba81b440f789664c6d38d510e1","deepnote_cell_type":"markdown"},"source":["* Talk about how youve worked with text, including regular expressions, unicode, etc.\n","* Describe which network science tools and data analysis strategies youve used, how those network science measures work, and why the tools youve chosen are right for the problem youre solving.\n","* How did you use the tools to understand your dataset?"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"19f05387f7c049ceaa92878978adb9bf","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Communities"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c569d2b5-f0a3-4394-9211-05ab0af7905f","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["We used NetworkX's modularity function to measure the quality of our network partitions. It measures this by looking at the edges intra-community with edges going outside the community and comparing it with our modulation and a random graph. The higher the modularity, the more non-random our communities are, and therefore we assume them to be better defined. That is, if we could achieve the modulation from pure chance, it most likely is not a community. "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d5adc740-89ec-4e3a-87a0-d18a11c6ecf5","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["One of the qualitative tools we used was the Netwulff graph visualization tool. It works by applying physics to the edges and nodes, such that we get a more dynamic graph, which pushes away nodes from each other and makes it easier to distinguish them from one another. "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"0abb357d-7528-4591-8677-683edc5651cf","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["In conjunction with the modularity, Netwulff allowed us to recognize, that partitioning the network into party-chamber configurations was a better partition than simply by party-party. Modularity then allowed us to confirm this hypothesis qualitatively."]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"72630b44e9344616a7406bcbc49b62fb","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":21354,"execution_start":1683392689759,"source_hash":"4b4b6f71"},"outputs":[{"data":{"text/plain":["(None, None)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#Example of the difference between the two\n","\n","G = pickle.load(open('directed_Multi_twitter_graph.pickle', 'rb'))\n","\n","def replace_na(graph,code_for_NaN = ''):\n","    #We turn NaNs into '' - this is only for Netwulf, as JSON cannot handle Nan values\n","    for node_key in graph.nodes:\n","        node = graph.nodes[node_key]\n","        for attribute in node.keys():\n","            if(type(node[attribute]) == float):\n","\n","                #If !(x>=0 or x<=0) => x is not a float, x is a nan value\n","                if not(node[attribute] >= 0 or node[attribute] <= 0):\n","                    node[attribute] = code_for_NaN\n","\n","def get_party_affiliation(G):\n","    normalizing_constant = len(G.nodes())\n","    affiliation_dict = defaultdict(int)\n","    replace_na(G,\"None\")\n","    for x,y in G.nodes(data=True):\n","        affiliation_dict[y[\"party\"]] += 1\n","\n","    for key in affiliation_dict.keys():\n","        affiliation_dict[key] = affiliation_dict[key]/normalizing_constant\n","\n","    return affiliation_dict\n","\n","# Specify node colors based on \"party\" attribute\n","for node in G.nodes:\n","    party = G.nodes[node][\"party\"]\n","    if party == \"D\":\n","        G.nodes[node][\"color\"] = \"blue\"\n","    elif party == \"R\":\n","        G.nodes[node][\"color\"] = \"red\"\n","    elif party == \"I\":\n","        G.nodes[node][\"color\"] = \"yellow\"\n","    else:\n","        G.nodes[node][\"color\"] = \"orange\"\n","\n","#We give each node a size based on their indegree\n","for node in G.nodes:\n","  G.nodes[node][\"size\"] = G.in_degree[node]\n","\n","\n","replace_na(G,\"\")\n","nw.visualize(G)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"18884266382742d98085b6a6ed757c23","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b3bae9c4-b2d8-4849-b4a2-0a4371025c45","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["In the data processing stage, we used the following regex formulas:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f277eddeca7d498885046388667b54d7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":3902962,"execution_start":1683387751339,"source_hash":"b6f60acb"},"outputs":[],"source":["RT_pattern = \"RT @[a-zA-Z0-9]*\" #Pattern for retweets -> it looks for \"RT @some_user_name\"\n","mentions_re = \"@[a-zA-Z0-9]*\" #Pattern for finding mentions -> it looks for \"@some_user_name\"\n","hashtags_re = \"#[a-zA-Z0-9]*\" #Pattern for finding hashtags -> it looks for \"#some_hastag\""]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"e6b2fa95-8105-4648-a865-8cd5f307c7cb","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["We used them in the following functions, which we then further used to extract the mentions and hashtags in the different tweet texts."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"47d40abb9dd74ea7a67d56c5cfe501f3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"d98dad8c"},"outputs":[],"source":["RT_pattern = \"RT @[a-zA-Z0-9]*\"\n","\n","def searchText(formula,twitter_message):\n","    try:\n","        matches = re.findall(formula,twitter_message)\n","        matches[0]\n","        return matches\n","    except:\n","        return None\n","\n","#Takes a tweet and returns a list of mentions that are in our accounts_dataFrame and their corresponding users' names\n","def get_Handles_Names(tweet):\n","    mentions_re = \"@[a-zA-Z0-9]*\"\n","    mentions_excl_RT_re = \"(?<!RT )@[a-zA-Z0-9]*\" #Will exclude all mentions that follow \"RT\"\n","    mentions = searchText(mentions_re,tweet)\n","    screen_names = []\n","    names_result = []\n","\n","    #If there is a mention\n","    if(mentions != None):\n","        mentions = [handle[1:] for handle in mentions] #Remove @\n","\n","        #Add to our dict, if the mentions exist in our dataFrame\n","        for idx,handle in enumerate(mentions):\n","            names = accounts_dataFrame[accounts_dataFrame['screen_name'] == handle]['name']\n","\n","            #If the handle exists in our filtered accounts_dataFrame\n","            if(len(names)>0):\n","                screen_names += [handle]\n","                names_result += list(names)\n","    if(len(names_result)>0):\n","        return screen_names,names_result\n","    else:\n","        return None,None\n","\n","def get_tweeter_screen_name(tweet):\n","    return tweet[\"screen_name\"]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b69703a1a9d54ec88ef08684c9c8302b","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Tools used in the tweet analysis section"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"e321be5bf0b64ef192e37f1e5378a91f","deepnote_cell_type":"markdown"},"source":["The mean tools used in the text analysis section is TF-IDF.  TF-IDF works by calculating the term frequency of each token in each community, and then weighting the term frequency by the inverse community frequency. As it is weighted by the community frequency it is a good way to get frequent words that are uniqe to each community. \n","We have used a special regex to clean and tokenise each tweet ```https?://\\S+|@\\w+|&\\w+|#\\w+|[^\\w\\s]|[^a-zA-Z\\s]+```. It is a set of 'or' criterias that each remove some unwanted item from the text. The first part removes all links, the next 3 parts remove the \"@\", \"&\",\"#\" symbols and the following text before a whitespace. The last part '[^\\w\\s]|[^a-zA-Z\\s]+' removes all none word characters.\n","\n","The full code is shown below"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f098779160f143f196e9e669cf797abc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"d560fa7b"},"outputs":[],"source":["# Loads tweets\n","textdf = pd.read_csv('data/textDF')\n","\n","# drop all columns except for the text and tweet_id column\n","textdf = textdf.drop(['Unnamed: 0','user_name','screen_name'],axis=1)\n","\n","textdf_dict = dict(zip(textdf.tweet_ID, textdf.text))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a5da7c8464d04574b1d30c781661c08e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"64083d1"},"outputs":[],"source":["def tokenize_tweet(raw_tweet: str):\n","    # Remove URLs, mentions, hashtags, punctuation, and any any non-alphabetic characters\n","    raw_tweet = re.sub(r'https?://\\S+|@\\w+|&\\w+|#\\w+|[^\\w\\s]|[^a-zA-Z\\s]+', '', raw_tweet.lower())\n","\n","    # tokenize -> converts each word to a list element\n","    tokens = nltk.word_tokenize(raw_tweet, language='english')\n","\n","    # loads in predifined english stop words\n","    stop_words = set(stopwords.words('english'))\n","    # ads domain Specific Stop words specific for twitter, this includes rt for retweets and qt for quoted tweets\n","    stop_words.update(['rt','qt','amp','lr','cm','im', 'today', 'us','must','would','thank','new','bcbra','cm','rsc','rnh'])\n","\n","    tokens = [token for token in tokens if token not in stop_words]\n","    \n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3432e3ea0ed44956ac00efd3125abd33","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"6686821"},"outputs":[],"source":["G_partitions_tokens = []\n","\n","for i, G_partition in enumerate(G_partitions):\n","    tweets_ids_in_partition = []\n","    tweets_tokenized_in_partition= []\n","\n","    for node in G_partition:\n","        for mention_node in G[node].keys():\n","            edge_data = G[node][mention_node]\n","            for tweet_id in edge_data['ID']:\n","                tweets_ids_in_partition.append(tweet_id)\n","\n","    for tweet_id in tqdm(tweets_ids_in_partition, desc=f'Tokenizing tweets {i+1} of {len(G_partitions)}'):\n","        tweet_text = textdf_dict[tweet_id]\n","        tokenized_tweet = tokenize_tweet(tweet_text)\n","        tweets_tokenized_in_partition += tokenized_tweet\n","    \n","    G_partitions_tokens.append(tweets_tokenized_in_partition)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"8a37b6842a9c4fd7b9102a5dfa4437c0","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Calculates TF for each community"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3da8451f57f04c37865734a453d07e1e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"9dd1795e"},"outputs":[],"source":["tf_dict = defaultdict(lambda: defaultdict(lambda: 0)) # term documents\n","\n","for i, partition_tokens in enumerate(G_partitions_tokens):\n","    # calculates TF for community\n","    for token in partition_tokens:\n","        tf_dict[token][i] += 1 / len(partition_tokens)\n","\n","top5_tokens = []\n","for partition_index in range(len(G_partitions_tokens)):\n","   tf_dict_copy = deepcopy(tf_dict) # makes deepcopy to not change default dict structure\n","   sorted_tokens = sorted(tf_dict_copy, key=lambda x: tf_dict_copy[x][partition_index], reverse=True)#\n","\n","   top5_tokens.append(sorted_tokens[:10])\n","\n","print(f'Top 10 tokens for partitions')\n","# print matrix as table using markdown\n","print('| Partition | Top 10 Tokens |')\n","print('| --- | --- |')\n","for i, top5 in enumerate(top5_tokens):\n","    print(f'| {i} | {\", \".join(top5)} |')\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"129ef8debc304374911741cd5abfb4a9","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Calculates IDF for each community"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"18a62c64480f453b9869dde0935eca83","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"9937e714"},"outputs":[],"source":["# IDF for each document\n","idf_dict = {}\n","num_docs = len(G_partitions_tokens)\n","for token in tf_dict:\n","    idf_dict[token] = np.log(num_docs / (len(tf_dict[token]))) # all tf_dict tokens apears so we do not need to add 1"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"5b7814f4-bb3d-4173-8922-d21e19e8d608","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Calculates tf-idf"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"803b7033a975411e9814e57162903089","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"5cb2f71d"},"outputs":[],"source":["# Calculate the TF-IDF weight for each term in each document\n","tfidf_dict = defaultdict(lambda: defaultdict(lambda: 0)) \n","for token in tf_dict:\n","    for community_index in tf_dict[token]:\n","        tfidf_dict[community_index][token] = tf_dict[token][community_index] * idf_dict[token]"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e85b667a136741fab5f0915565f498b3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"86cb8979"},"outputs":[],"source":["# Prints top 10 TF-IDF for each community \n","top_10_TFIDF = []\n","for community_index in tfidf_dict:\n","    top_10_TFIDF.append(sorted(tfidf_dict[community_index], key=lambda x: tfidf_dict[community_index][x],reverse=True)[:10])\n","    #print(\"Top 10 TF-IDF words for community: \",community_index, )\n","\n","print(f'Top 10 TF-IDF for partitions')\n","# print matrix as table using markdown\n","print('| Partition | Top 10 TF-IDF |')\n","print('| --- | --- |')\n","for i, top10 in enumerate(top_10_TFIDF):\n","    print(f'| {i} | {\", \".join(top10)} |')"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"37d61a3a50504c16b96aa568155b4482","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Creates wordcloud based on TF-IDF (to get better words than simply the most frequent)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1ce0b72d2f18485e88d258ff434210b6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"526d27ac"},"outputs":[],"source":["# Generate a word cloud image\n","fig, axs = plt.subplots(3,2, figsize=(20,10))\n","#fig.subplots_adjust(hspace=-.2)\n","fig.subplots_adjust(wspace=-0.5)\n","\n","for i, community_index in enumerate(tfidf_dict):\n","    #wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(['sadf','sadf'])\n","    wordcloud = WordCloud(max_font_size=40, max_words=50, background_color=\"white\").generate_from_frequencies(tfidf_dict[community_index])\n","\n","    axs[i // 2, i % 2].imshow(wordcloud, interpolation=\"bilinear\")\n","    axs[i // 2, i% 2].axis(\"off\")\n","    axs[i // 2, i% 2].set_title(f'Community: {community_index}', fontsize=20)\n","\n","    if i == 6:\n","        break\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"0ff81023e615479186d7edd2e813c1f2","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Finally the biggest nodes in each comminty"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"611f64281f8a42cbbdb43e794bc3fd74","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"9c9dde5e"},"outputs":[],"source":["G_partitions\n","\n","top_3_nodes_per_community = []\n","\n","for i, G_partition in enumerate(G_partitions):\n","    top_3_nodes_per_community.append(sorted(G_partition, key=lambda x: G.degree(x,weight='weight'), reverse=True)[:3])\n","\n","print(f'Top 3 accounts for partitions')\n","# print matrix as table using markdown\n","print('| Partition | Top 3 accounts |')\n","print('| --- | --- |')\n","for i, top10 in enumerate(top_3_nodes_per_community):\n","    print(f'| {i} | {\", \".join(top10)} |')"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"550f5215-08cb-4ede-84d7-cc37bd8cfefc","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"6e8058a6-34aa-452c-905c-2de01ee35b9d","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["In our community section of the webpage, we used "]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"7cf21635af674c08aeadbdec8c7a66e8","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Discussion. Think critically about your creation"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"7a324458767f45fc98a60ba7f2b70caa","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["From our analysis we have found a level of polarisation in the US Goverment. We think that this is a worying trend"]},{"attachments":{},"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2916efa0-c445-43ca-8dcd-0a1bcdbbd016' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"7f42fc8a4982424eb02a3c5343479424","language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
